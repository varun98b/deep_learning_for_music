# deep_learning_for_music

This project explores and attempts to recreate the intersection of deep learning and music composition through the development of a neural network model capable of generating music in the style of J.S. Bach, as first seen in the paper “Deep Learning for Music” by Huang and Wu (2016).  Utilizing Long Short-Term Memory (LSTM) networks, a form of recurrent neural networks known for their efficacy in learning sequences, the model was trained on a corpus of MIDI files to capture the intricacies of Bach's compositions.  The training involved processing the MIDI files to extract musical elements—notes and chords—and preparing sequential data that enabled the network to learn patterns and predict subsequent musical notes.  Major findings from the project include the successful identification and generation of music sequences that bear Bach's compositional signatures.  The generation of music was followed by a visualization step using t-Distributed Stochastic Neighbor Embedding (t-SNE), which provided insights into the learned features and the model's internal representations of music data.

### Dataset
The replacement dataset - found at bachcentral.com - contained 228 midi files of Bach’s works.  Unfortunately, during preprocessing of data, it was found that 88 of these files were improperly formatted, leaving only 140 usable files for our dataset.  Across these files, 62363 notes/chords were sourced across 204 unique pitches/chords.  Figure 1 shows the frequencies of each note/chord, truncated to the highest 50 for graph readability.  Of note from this visualization is that well over 85% of this dataset consisted of only the top-40 most pitches/chords.  This was, unfortunately, substantially less than what was utilized in the dataset reported by Huang and Wu.  These notes/chords were appended sequentially in the order sourced file-by-file, and this total list was used to generate a supervised training dataset of 62313 50-consecutive-element sequences which were each mapped to the note immediately following the sequence for labeling purposes. 

![image](https://github.com/varun98b/deep_learning_for_music/assets/51908568/1a31699e-4083-4084-b261-614b0cf53081)

### Experiment
The experiment was performed through a pipeline containing the dataset loading and preprocessing into sequence-note mappings, model creation, and the model training for the 50 epochs described in Huang and Wu’s paper.  The training metrics used in that paper were never outlined in specifics, so categorical cross-entropy loss was monitored for determining the best training fit.  Similarly, evaluation metrics for music generation were not outlined beyond a subjective listening comparison test, so the extent of evaluation was whether the midi file generated from the model post-training seemed comparable to Bach’s works. 

The original paper had, however, performed t-distributed stochastic neighbor embedding (t-SNE) visualization upon the dataset.  The original visualization for the paper’s Bach dataset and the visualization for this replacement dataset can be seen in Figure.

![image](https://github.com/varun98b/deep_learning_for_music/assets/51908568/cffa7d9d-ba6f-4ea7-8cfe-d69d18b58a0e)
